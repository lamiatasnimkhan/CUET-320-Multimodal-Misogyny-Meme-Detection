{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHZr-RGkN_IC"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch pandas scikit-learn jieba"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing Function\n",
        "]"
      ],
      "metadata": {
        "id": "c6ud14NMPFmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load datasets\n",
        "train_csv = pd.read_csv('/kaggle/input/misogyny-meme-detection/train.csv')\n",
        "dev_csv = pd.read_csv('/kaggle/input/misogyny-meme-detection/dev.csv')\n",
        "test_csv= pd.read_csv('/kaggle/input/misogyny-meme-detection/test.csv')\n",
        "\n",
        "# Predefined list of Chinese stopwords\n",
        "chinese_stopwords = set([\n",
        "    \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"\n",
        "])\n",
        "\n",
        "\n",
        "# Preprocess function for Chinese text\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Remove URLs, special characters, etc.\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "\n",
        "    # Tokenize using jieba\n",
        "    words = jieba.lcut(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords and len(word) > 1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'transcriptions' column\n",
        "train_csv['transcriptions'] = train_csv['transcriptions'].apply(lambda x: preprocess_text(x, chinese_stopwords))\n",
        "dev_csv['transcriptions'] = dev_csv['transcriptions'].apply(lambda x: preprocess_text(x, chinese_stopwords))\n",
        "test_csv['transcriptions'] = test_csv['transcriptions'].apply(lambda x: preprocess_text(x, chinese_stopwords))\n",
        "\n",
        "# Map string labelsss to integers\n",
        "labels_map = {\n",
        "    \"Misogyny\": 1,\n",
        "    \"Not-Misogyny\": 0\n",
        "}\n",
        "train_csv['labels'] = train_csv['labels'].map(labels_map)\n",
        "dev_csv['labels'] = dev_csv['labels'].map(labels_map)\n",
        "\n",
        "# Verify the labelsss\n",
        "print(\"Train labels:\", train_csv['labels'].unique())\n",
        "print(\"Dev labels:\", dev_csv['labels'].unique())"
      ],
      "metadata": {
        "id": "5A_6q3AiPDvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Preprocessing"
      ],
      "metadata": {
        "id": "bfTkIvPNPMv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2"
      ],
      "metadata": {
        "id": "Md_6l6utPB3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import imghdr\n",
        "import re\n",
        "\n",
        "def load_images_to_dataframe(image_dir):\n",
        "    data = []\n",
        "\n",
        "    for file_name in os.listdir(image_dir):\n",
        "        file_path = os.path.join(image_dir, file_name)\n",
        "\n",
        "        # Check if it's an image file\n",
        "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')):\n",
        "            try:\n",
        "                img = None  # Initialize image variable\n",
        "\n",
        "                # Try reading the image using OpenCV\n",
        "                img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "                # If OpenCV fails, try PIL\n",
        "                if img is None:\n",
        "                    print(f\"OpenCV failed to read {file_name}, trying PIL...\")\n",
        "                    img = Image.open(file_path).convert(\"RGB\")  # Try using PIL instead\n",
        "                    img = img.resize((224, 224))\n",
        "                    img = np.array(img)\n",
        "                else:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "                    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                # Apply brightness/contrast adjustment\n",
        "                alpha = 1.2\n",
        "                beta = 20\n",
        "                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "                # Keep the full image name with extension\n",
        "                image_name = file_name  # Keep the full filename including extension (e.g., 1549.jpg)\n",
        "\n",
        "                # Append the image data and its filename to the list\n",
        "                data.append({'image_name': image_name, 'image_data': img})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "cPPTnYY8PR6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = load_images_to_dataframe('/kaggle/input/misogyny-meme-detection/train-20250320T200243Z-001/train')\n",
        "df_test = load_images_to_dataframe('/kaggle/input/misogyny-meme-detection/test-20250327T173652Z-001/test')\n",
        "df_dev = load_images_to_dataframe('/kaggle/input/misogyny-meme-detection/dev-20250321T154338Z-001/dev')"
      ],
      "metadata": {
        "id": "SmAqoMrcPVgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.merge(train_csv, df_train, on='image_name', how='inner')\n",
        "dev = pd.merge(dev_csv, df_dev, on='image_name', how='inner')\n",
        "test = pd.merge(test_csv, df_test, on='image_name', how='inner')"
      ],
      "metadata": {
        "id": "6WlySOkmPX0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "sX5zPHx_PZ0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "augmented_data = []\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "img_augmentations = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=0.5),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandomPosterize(bits=4),\n",
        "\n",
        "])\n",
        "\n",
        "for idx, row in train.iterrows():\n",
        "    image_data = row['image_data']\n",
        "    text = row['transcriptions']\n",
        "    labels = row['labels']\n",
        "    image_name = row['image_name']\n",
        "\n",
        "    if labels == 1:\n",
        "        cnt += 1\n",
        "        # print(cnt)\n",
        "\n",
        "\n",
        "        image = Image.fromarray(image_data.astype('uint8'))\n",
        "        img_aug = img_augmentations(image)\n",
        "        img_aug = np.array(img_aug)\n",
        "\n",
        "        augmented_data.append({\n",
        "            'image_data': img_aug,\n",
        "            'labels': labels,\n",
        "            'image_name': image_name,\n",
        "            'transcriptions': text\n",
        "        })\n",
        "\n",
        "\n",
        "        image = Image.fromarray(image_data.astype('uint8'))\n",
        "        img_aug = img_augmentations(image)\n",
        "        img_aug = np.array(img_aug)\n",
        "\n",
        "        augmented_data.append({\n",
        "            'image_data': img_aug,\n",
        "            'labels': labels,\n",
        "            'image_name': image_name,\n",
        "            'transcriptions': text\n",
        "        })\n",
        "\n",
        "\n",
        "augmented_df = pd.DataFrame(augmented_data)\n",
        "\n",
        "\n",
        "train = pd.concat([train, augmented_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "XCyamYWAPb-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChineseBERT+ResNet"
      ],
      "metadata": {
        "id": "lgECiGLiPzDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        label = self.df.loc[idx, 'labels']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return encoding, image, torch.tensor(label)\n",
        "\n",
        "\n",
        "# Load Chinese BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
        "bert_model = BertModel.from_pretrained('hfl/chinese-bert-wwm')\n",
        "\n",
        "# Load ResNet-50 model\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model.fc = nn.Identity()  # Remove classification head\n",
        "\n",
        "# Image transformation\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class MultiModal(nn.Module):\n",
        "    def __init__(self, bert_model, resnet_model, num_classes):\n",
        "        super(MultiModal, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.resnet_model = resnet_model\n",
        "        self.fc = nn.Linear(768 + 2048, num_classes)  # 768 from BERT, 2048 from ResNet-50\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        bert_pool = torch.mean(bert_output, 1)\n",
        "\n",
        "        resnet_output = self.resnet_model(images)\n",
        "\n",
        "        combined_features = torch.cat((bert_pool, resnet_output), dim=1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n",
        "\n",
        "\n",
        "train_dataset = MultiModalDataset(train, tokenizer, image_transform)\n",
        "dev_dataset = MultiModalDataset(dev, tokenizer, image_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16)\n",
        "\n",
        "num_classes = len(train['labels'].unique())\n",
        "model = MultiModal(bert_model, resnet_model, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "id": "PqHGwicdPefI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * np.mean(np.array(y_true) == np.array(y_pred))\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)"
      ],
      "metadata": {
        "id": "Te2xNubNP2Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "oz28a1dzQAQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding='max_length', truncation=True, max_length=512, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return encoding, image\n",
        "\n",
        "test_dataset = TestDataset(test, tokenizer, image_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        pixel_values = batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "\n",
        "\n",
        "test['predictions'] = test_predictions\n",
        "\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'image_name': test['image_name'],\n",
        "    'predictions': test_predictions\n",
        "})\n",
        "\n",
        "output_file_path = '/kaggle/working/chineseBert_ResNet_predictions.csv'\n",
        "predictions_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "crdXF7CGQCip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove everything after the first dot (extension)\n",
        "test['image_name'] = test['image_name'].str.split('.').str[0]\n",
        "\n",
        "# Convert image names to numeric values (assuming they are numeric)\n",
        "test['image_name'] = pd.to_numeric(test['image_name'], errors='coerce')\n",
        "\n",
        "# Create a DataFrame for predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'image_name': test['image_name'],\n",
        "    'predictions': test_predictions\n",
        "})\n",
        "\n",
        "# Sort by 'image_name' numerically\n",
        "predictions_df = predictions_df.sort_values(by='image_name')\n",
        "\n",
        "# Remove the headers (columns) from the CSV\n",
        "predictions_df.to_csv('/kaggle/working/chineseBert_ResNet_predictions_submission.csv', header=False, index=False)\n",
        "\n",
        "print(\"Predictions saved without headers and sorted numerically by image_name\")"
      ],
      "metadata": {
        "id": "x1sXmRtsQFRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet+ ChineseBERT"
      ],
      "metadata": {
        "id": "SEDHCWf3Qa_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        label = self.df.loc[idx, 'labels']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "\n",
        "        # Tokenize text (Chinese BERT)\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Process image\n",
        "        image = Image.fromarray(image)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return encoding, image, torch.tensor(label)\n",
        "\n",
        "# Load Chinese BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
        "bert_model = BertModel.from_pretrained('hfl/chinese-bert-wwm')\n",
        "\n",
        "# Load DenseNet-121\n",
        "densenet_model = models.densenet121(pretrained=True)\n",
        "densenet_model.classifier = nn.Identity()  # Remove classification head\n",
        "\n",
        "# Image transformation (same as ResNet)\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, bert_model, densenet_model, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.densenet_model = densenet_model\n",
        "\n",
        "        # BERT hidden size = 768, DenseNet-121 output = 1024\n",
        "        self.fc = nn.Linear(768 + 1024, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text features (BERT)\n",
        "        bert_output = self.bert_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        bert_pooled = torch.mean(bert_output, dim=1)  # Pooling (mean)\n",
        "\n",
        "        # Image features (DenseNet)\n",
        "        densenet_features = self.densenet_model(images)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat((bert_pooled, densenet_features), dim=1)\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "# Example usage (assuming `train` and `dev` DataFrames exist)\n",
        "num_classes = len(train['labels'].unique())\n",
        "model = MultiModalModel(bert_model, densenet_model, num_classes)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = MultiModalDataset(train, tokenizer, image_transform)\n",
        "dev_dataset = MultiModalDataset(dev, tokenizer, image_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {100 * correct / total}%\")"
      ],
      "metadata": {
        "id": "YJGY4K5TQaPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in dev_loader:\n",
        "        # Move batch to device (handles edge cases like single-sample batches)\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class indices\n",
        "\n",
        "        # Ensure tensors are CPU-side for sklearn\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100  # More reliable than np.mean\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')  # Macro-average for imbalanced classes\n",
        "    f1_weighted = f1_score(y_true, y_pred, average='weighted')  # Weighted-average (class-sensitive)\n",
        "    class_report = classification_report(y_true, y_pred, digits=4)  # 4 decimal places\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-score (Weighted): {f1_weighted:.4f}\")\n",
        "    print(\"Detailed Classification Report:\")\n",
        "    print(class_report)"
      ],
      "metadata": {
        "id": "eveF1uQcQIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "Ezg4DwIkQj9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os  # For path handling\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df.reset_index(drop=True)  # Ensure consistent indexing\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "--int(f\"Predictions saved to {output_path}\")"
      ],
      "metadata": {
        "id": "oRN9GqEmQnxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First ensure the column contains strings by converting it\n",
        "test['image_name'] = test['image_name'].astype(str)\n",
        "\n",
        "# Now safely remove everything after the first dot\n",
        "test['image_name'] = test['image_name'].str.split('.').str[0]\n",
        "\n",
        "# Convert to numeric (if needed)\n",
        "test['image_name'] = pd.to_numeric(test['image_name'], errors='coerce')\n",
        "\n",
        "# Drop any rows that couldn't be converted to numbers\n",
        "test = test.dropna(subset=['image_name'])\n",
        "\n",
        "# Create predictions DataFrame\n",
        "predictions_df = pd.DataFrame({\n",
        "    'image_name': test['image_name'].astype(int),\n",
        "    'predictions': test_predictions\n",
        "})\n",
        "\n",
        "# Sort numerically\n",
        "predictions_df = predictions_df.sort_values('image_name')\n",
        "\n",
        "# Save without headers\n",
        "predictions_df.to_csv('/kaggle/working/chineseBert_DenseNet_predictions_submission.csv',\n",
        "                     header=False,\n",
        "                     index=False)"
      ],
      "metadata": {
        "id": "fJWud_exQqiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet+XLM-R"
      ],
      "metadata": {
        "id": "y_7doFzqRe_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        label = self.df.loc[idx, 'labels']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "        image = Image.fromarray(image)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return encoding, image, torch.tensor(label)\n",
        "\n",
        "\n",
        "# Load XLM-R tokenizer and model\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "xlmr_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "# Load DenseNet model\n",
        "densenet_model = models.densenet121(pretrained=True)\n",
        "densenet_model.classifier = nn.Identity()  # Remove classification head\n",
        "\n",
        "# Image transformation\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class MultiModal(nn.Module):\n",
        "    def __init__(self, xlmr_model, densenet_model, num_classes):\n",
        "        super(MultiModal, self).__init__()\n",
        "        self.xlmr_model = xlmr_model\n",
        "        self.densenet_model = densenet_model\n",
        "        self.fc = nn.Linear(768 + 1024, num_classes)  # 768 from XLM-R, 1024 from DenseNet-121\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        xlmr_output = self.xlmr_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        xlmr_pool = torch.mean(xlmr_output, 1)\n",
        "\n",
        "        densenet_output = self.densenet_model(images)\n",
        "\n",
        "        combined_features = torch.cat((xlmr_pool, densenet_output), dim=1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n",
        "train_dataset = MultiModalDataset(train, tokenizer, image_transform)\n",
        "dev_dataset = MultiModalDataset(dev, tokenizer, image_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16)\n",
        "\n",
        "num_classes = len(train['labels'].unique())\n",
        "model = MultiModal(xlmr_model, densenet_model, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    print(f\"Accuracy: {accuracy}%\")\n",
        "    print(f\"Macro F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "qL7TIsGvQuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        images = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "        # Store true and predicted labels\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Compute accuracy and F1-score\n",
        "    accuracy = 100 * np.mean(np.array(y_true) == np.array(y_pred))\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)"
      ],
      "metadata": {
        "id": "V-7M0aneRlPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'transcriptions']\n",
        "        image = self.df.loc[idx, 'image_data']\n",
        "\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding='max_length', truncation=True, max_length=512, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return encoding, image\n",
        "\n",
        "test_dataset = TestDataset(test, tokenizer, image_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "\n",
        "        input_ids = batch[0]['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch[0]['attention_mask'].squeeze(1).to(device)\n",
        "        pixel_values = batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, pixel_values)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "\n",
        "\n",
        "test['predictions'] = test_predictions\n",
        "\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'image_name': test['image_name'],\n",
        "    'predictions': test_predictions\n",
        "})\n",
        "\n",
        "output_file_path = '/kaggle/working/xlmr_denseNet.csv'\n",
        "predictions_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "pzfz3PowRslj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove everything after the first dot (extension)\n",
        "test['image_name'] = test['image_name'].str.split('.').str[0]\n",
        "\n",
        "# Convert image names to numeric values (assuming they are numeric)\n",
        "test['image_name'] = pd.to_numeric(test['image_name'], errors='coerce')\n",
        "\n",
        "# Create a DataFrame for predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'image_name': test['image_name'],\n",
        "    'predictions': test_predictions\n",
        "})\n",
        "\n",
        "# Sort by 'image_name' numerically\n",
        "predictions_df = predictions_df.sort_values(by='image_name')\n",
        "\n",
        "# Remove the headers (columns) from the CSV\n",
        "predictions_df.to_csv('/kaggle/working/chineseBert_ResNet_predictions_submission.csv', header=False, index=False)\n",
        "\n",
        "print(\"Predictions saved without headers and sorted numerically by image_name\")"
      ],
      "metadata": {
        "id": "u9bnf2OURwvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}