{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUlVm4CTEuo2"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch pandas scikit-learn jieba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/kaggle/input/misogyny/train.csv')\n",
        "dev_df = pd.read_csv('/kaggle/input/misogyn/dev.csv')\n",
        "\n",
        "# Predefined list of Chinese stopwords\n",
        "chinese_stopwords = set([\n",
        "    \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"\n",
        "])\n",
        "\n",
        "\n",
        "# Preprocess function for Chinese text\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Remove URLs, special characters, etc.\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "\n",
        "    # Tokenize using jieba\n",
        "    words = jieba.lcut(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords and len(word) > 1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'transcriptions' column\n",
        "train_df['transcriptions'] = train_df['transcriptions'].apply(lambda x: preprocess_text(x, chinese_stopwords))\n",
        "dev_df['transcriptions'] = dev_df['transcriptions'].apply(lambda x: preprocess_text(x, chinese_stopwords))\n",
        "\n",
        "# Map string labels to integers\n",
        "label_map = {\n",
        "    \"Misogyny\": 1,\n",
        "    \"Not-Misogyny\": 0\n",
        "}\n",
        "train_df['labels'] = train_df['labels'].map(label_map)\n",
        "dev_df['labels'] = dev_df['labels'].map(label_map)\n",
        "\n",
        "# Verify the labels\n",
        "print(\"Train Labels:\", train_df['labels'].unique())\n",
        "print(\"Dev Labels:\", dev_df['labels'].unique())"
      ],
      "metadata": {
        "id": "BeaHNoazGowF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChineseBERT"
      ],
      "metadata": {
        "id": "4VJekxVKIQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Custom Dataset class\n",
        "class ChineseTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]  # Ensure this is an integer\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove batch dimension\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)  # Ensure label is an integer\n",
        "        }\n",
        "\n",
        "# Load tokenizers using AutoTokenizer\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
        "\n",
        "# Create datasets for BERT and RoBERTa\n",
        "train_dataset_bert = ChineseTextDataset(train_df['transcriptions'], train_df['labels'], bert_tokenizer)\n",
        "dev_dataset_bert = ChineseTextDataset(dev_df['transcriptions'], dev_df['labels'], bert_tokenizer)\n",
        "\n",
        "train_dataset_roberta = ChineseTextDataset(train_df['transcriptions'], train_df['labels'], roberta_tokenizer)\n",
        "dev_dataset_roberta = ChineseTextDataset(dev_df['transcriptions'], dev_df['labels'], roberta_tokenizer)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_bert = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
        "dev_loader_bert = DataLoader(dev_dataset_bert, batch_size=16)\n",
        "\n",
        "train_loader_roberta = DataLoader(train_dataset_roberta, batch_size=16, shuffle=True)\n",
        "dev_loader_roberta = DataLoader(dev_dataset_roberta, batch_size=16)"
      ],
      "metadata": {
        "id": "XriS7zVVGuyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChineseBERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(ChineseBERTClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.2)  # Dropout for regularization\n",
        "        self.fc = nn.Linear(768, num_classes)  # BERT hidden size is 768\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = BertModel.from_pretrained('bert-base-chinese')\n",
        "\n",
        "# Initialize the classifier\n",
        "num_classes = len(train_df['labels'].unique())  # Number of classes\n",
        "bert_classifier = ChineseBERTClassifier(bert_model, num_classes)"
      ],
      "metadata": {
        "id": "FjxE_FLsGyFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def train_and_evaluate(model, train_loader, dev_loader, model_name):\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Added weight decay\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    total_steps = len(train_loader) * 10  # 10 epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Number of epochs\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"{model_name} - Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f\"{model_name} - Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(f\"{model_name} - F1 Score: {f1_score(y_true, y_pred, average='macro')}\")\n",
        "\n",
        "# Train and evaluate BERT\n",
        "train_and_evaluate(bert_classifier, train_loader_bert, dev_loader_bert, \"BERT\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wMl-g9y-G3kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "# Save BERT model\n",
        "torch.save(bert_classifier.state_dict(), 'chinese_bert_misogyny_model.pth')"
      ],
      "metadata": {
        "id": "yrpjvz_IG7Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mBERT"
      ],
      "metadata": {
        "id": "4NyNOEgQIKy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load mBERT tokenizer and model\n",
        "mbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "mbert_model = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Create datasets for mBERT\n",
        "train_dataset_mbert = ChineseTextDataset(train_df['transcriptions'], train_df['labels'], mbert_tokenizer)\n",
        "dev_dataset_mbert = ChineseTextDataset(dev_df['transcriptions'], dev_df['labels'], mbert_tokenizer)\n",
        "\n",
        "# Create dataloaders for mBERT\n",
        "train_loader_mbert = DataLoader(train_dataset_mbert, batch_size=16, shuffle=True)\n",
        "dev_loader_mbert = DataLoader(dev_dataset_mbert, batch_size=16)\n",
        "\n",
        "# Define mBERT classifier\n",
        "class ChineseMBERTClassifier(nn.Module):\n",
        "    def __init__(self, mbert_model, num_classes):\n",
        "        super(ChineseMBERTClassifier, self).__init__()\n",
        "        self.mbert = mbert_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(768, num_classes)  # mBERT hidden size is 768\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.mbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize mBERT classifier\n",
        "mbert_classifier = ChineseMBERTClassifier(mbert_model, num_classes)\n",
        "\n",
        "# Train and evaluate mBERT\n",
        "train_and_evaluate(mbert_classifier, train_loader_mbert, dev_loader_mbert, \"mBERT\")\n",
        "\n",
        "# Save mBERT model\n",
        "torch.save(mbert_classifier.state_dict(), 'chinese_mbert_misogyny_model.pth')"
      ],
      "metadata": {
        "id": "fBFhKkuPG-3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XLM-R"
      ],
      "metadata": {
        "id": "mDfBxOdsIFm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load XLM-R tokenizer and model\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "xlmr_model = AutoModel.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "# Create datasets for XLM-R\n",
        "train_dataset_xlmr = ChineseTextDataset(train_df['transcriptions'], train_df['labels'], xlmr_tokenizer)\n",
        "dev_dataset_xlmr = ChineseTextDataset(dev_df['transcriptions'], dev_df['labels'], xlmr_tokenizer)\n",
        "\n",
        "# Create dataloaders for XLM-R\n",
        "train_loader_xlmr = DataLoader(train_dataset_xlmr, batch_size=16, shuffle=True)\n",
        "dev_loader_xlmr = DataLoader(dev_dataset_xlmr, batch_size=16)\n",
        "\n",
        "# Define XLM-R classifier\n",
        "class ChineseXLMRClassifier(nn.Module):\n",
        "    def __init__(self, xlmr_model, num_classes):\n",
        "        super(ChineseXLMRClassifier, self).__init__()\n",
        "        self.xlmr = xlmr_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(768, num_classes)  # XLM-R hidden size is 768\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.xlmr(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize XLM-R classifier\n",
        "xlmr_classifier = ChineseXLMRClassifier(xlmr_model, num_classes)\n",
        "\n",
        "# Train and evaluate XLM-R\n",
        "train_and_evaluate(xlmr_classifier, train_loader_xlmr, dev_loader_xlmr, \"XLM-R\")\n",
        "\n",
        "# Save XLM-R model\n",
        "torch.save(xlmr_classifier.state_dict(), 'chinese_xlmr_misogyny_model.pth')"
      ],
      "metadata": {
        "id": "jTv1axAHH9Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooo_8BBQICPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}